# rag_api.py
import os
from fastapi import FastAPI
from pydantic import BaseModel
from langchain_groq import ChatGroq
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate



 
# 1. Setup cl√© API Groq
os.environ["GROQ_API_KEY"] = "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"

# 2. Initialiser LLM Groq
llm = ChatGroq(model_name="llama3-70b-8192")


# 3. Charger les embeddings (m√™me mod√®le que pour cr√©ation index)
embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# 4. Charger l'index FAISS local avec embeddings
vectorstore = FAISS.load_local("faiss_index", embedding, allow_dangerous_deserialization=True)

# 5. Cr√©er un Retriever avec k r√©duit √† 10 documents
retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# 6. M√©moire conversationnelle avec une fen√™tre de 3 derniers √©changes
memory = ConversationBufferWindowMemory(k=3, memory_key="chat_history", return_messages=True)

from langchain.prompts import PromptTemplate

template_text = """
Tu es un assistant expert en orientation universitaire √† la Facult√© des Sciences Dhar Mahraz (FSDM).
Tu r√©ponds aux questions sur les masters, les conditions d'admission, les sp√©cialisations, et tout ce qui concerne les masters de cette facult√©.

Ta r√©ponse doit √™tre claire, concise et directe.
Ne mentionne jamais que tu utilises des documents, ni que ta r√©ponse provient d'une recherche.
R√©pond comme un conseiller expert qui conna√Æt parfaitement les masters de la FSDM.

Contexte:
{context}

Question:
{question}

R√©ponse :
"""
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=template_text
)

# 8. Construire la cha√Æne conversationnelle avec retrieval + m√©moire + prompt
conv_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
    combine_docs_chain_kwargs={"prompt": prompt}
)


# 9. Fonction d'interrogation simplifi√©e
def ask_question(question: str):
    result = conv_chain.run(question)
    print(f"\nüí¨ Question : {question}")
    print(f"üß† R√©ponse : {result}\n")

# 10. Exemple d'utilisation
if __name__ == "__main__":
    ask_question("Quels sont les masters accessibles ?")
    ask_question("Et les conditions d'admission ?")
    ask_question("Quels masters sont les plus demand√©s ?")

from fastapi.middleware.cors import CORSMiddleware
app = FastAPI()

# --- Configuration CORS ---
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # √† restreindre en prod (ex: ["https://ton-frontend.vercel.app"])
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Sch√©ma de requ√™te ---
class QueryRequest(BaseModel):
    question: str

# --- Endpoint RAG ---
@app.post("/query")
def ask_question_api(request: QueryRequest):
    answer = conv_chain.run(request.question)
    return {"answer": answer}